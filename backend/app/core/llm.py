import requests

from app.core.config import settings

LLAMA_SERVER_URL = settings.LLAMA_SERVER_URL


def run_llm(prompt: str) -> str:
    payload = {
        "model": settings.LLM_MODEL,
        "prompt": prompt,
        "temperature": 0.4,
        "max_tokens": 200,
        "n_predict": 200,
        "stream": False
    }

    try:
        response = requests.post(
            LLAMA_SERVER_URL,
            json=payload,
            timeout=settings.LLM_TIMEOUT
        )
    except requests.exceptions.Timeout:
        raise TimeoutError("Model response timed out.")
    except requests.exceptions.RequestException as e:
        raise RuntimeError(f"LLM server connection failed: {str(e)}")

    if response.status_code != 200:
        raise RuntimeError(f"LLM server returned error: {response.text}")

    data = response.json()

    if "choices" not in data or not data["choices"]:
        raise ValueError("No response generated by the model.")

    return data["choices"][0]["text"].strip()
